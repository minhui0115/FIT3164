<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>What is Hallucination in AI?</title>
    <style>
         body {
            font-family: Arial, sans-serif;
            background-image: url('photo/photo_ai-hallucinations.webp');
            background-size: cover;
            background-position: center;
            background-repeat: no-repeat;
            background-attachment: fixed;
            margin: 0;
            padding: 20px;
            color: white;
        }
        header {
            background-color: #333;
            color: white;
            padding: 10px 0;
            text-align: center;
        }
        h1 {
            font-size: 2.5em;
        }
        section {
            background-color: white;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        }
        h2 {
            color: #333;
        }
        p {
            line-height: 1.6;
            color: #555;
        }
    </style>
</head>
<body>
    <header>
        <h1>What is Hallucination in AI?</h1>
    </header>
    
    <section>
        <h2>Definition</h2>
        <p>In the context of artificial intelligence (AI), "hallucination" refers to situations where a machine learning model, such as a language model or image generator, produces information that seems plausible but is factually incorrect or nonsensical.</p>
    </section>

    <section>
        <h2>Types of AI Hallucinations</h2>
        <p>Hallucinations can occur in various forms of AI:</p>
        <ul>
            <li><strong>Text-based AI:</strong> A language model might generate sentences that sound reasonable but are factually wrong.</li>
            <li><strong>Image-based AI:</strong> An image generator could produce visual artifacts that donâ€™t exist in reality, such as extra limbs on a person.</li>
        </ul>
    </section>

    <section>
        <h2>Why Hallucinations Happen</h2>
        <p>AI models, particularly those using deep learning techniques, are trained on vast amounts of data. However, when they are asked to generate new data (such as text or images), they may create outputs based on patterns they've learned that do not reflect reality. This can happen because of insufficient training data, ambiguities, or overfitting to certain patterns.</p>
    </section>

    <section>
        <h2>Consequences and Challenges</h2>
        <p>AI hallucinations can have serious consequences, particularly in fields like healthcare, where incorrect information can lead to harmful decisions. Managing hallucinations remains an important challenge in AI development.</p>
    </section>

    <footer>
        <p>Created by [Team MDS 16] &copy; 2024</p>
    </footer>
</body>
</html>
